{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b55c6b0-a531-484d-be51-29fd7d95e989",
   "metadata": {},
   "source": [
    "<font size=7>Reliability of the salience features and object categories<font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abeed6b-f2c9-4f1e-97e1-8c26f1f6e3d7",
   "metadata": {},
   "source": [
    "<font size=3>Finally, we want to look at the split-half reliability of the salience features and the object categories in our data set. The **extract_fix_split_half()** function provides a dataframe with two columns for each saliency feature and object category (color, intensity, orientation, text, face, taste, motion, touched). These correspond to values from even and odd images. Each row contains the data of one subject.\n",
    "\n",
    "* <font size=3>Calculate the split-half reliability for all saliency features and object categories. How do you assess the reliabilities?<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831b081a-99e8-4ec9-8e24-616a4827bbb7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from PIL import Image\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "import ITTISaliencyLib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "IMAGE_FOLDER = 'images_task_F'\n",
    "image_files = [f for f in os.listdir(IMAGE_FOLDER) if f.endswith('.jpg')]\n",
    "RESULTS_FOLDER_FIX = 'data_task_C/data_fix/'\n",
    "RESULTS_FOLDER_SACC = 'data_task_C/data_sacc/'\n",
    "#SALIENCY_MAP_DICT = 'data_task_F/saliency_map_dict.npy'\n",
    "OSIE_FILE = 'data_task_F/attrs_OSIE_40.mat'\n",
    "#SUBJECT_LIST = [sub[:2] for sub in os.listdir(RESULTS_FOLDER_FIX)if sub != '.DS_Store']\n",
    "#print(SUBJECT_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a7547-5904-40da-8364-03921f9dd311",
   "metadata": {},
   "source": [
    "***The cell below might take a long time to execute:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a79568c1-1f90-475e-9bda-6b9c9597d00b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_saliency_map_dict():\n",
    "    #images_path = \"/Users/patrikpolgari/Documents/Postdoc_Marburg/student_supervision/Teaching/B-NP2_Übung/Code/Task_C/OSIE_40/images\"\n",
    "    #image_list = []\n",
    "\n",
    "    #os.chdir(images_path)\n",
    "    #os.chdir(IMAGE_FOLDER)\n",
    "    #for file in glob.glob(\"*.jpg\"):\n",
    "    #    image_list.append(file)\n",
    "\n",
    "    saliency_map_dict = {}\n",
    "\n",
    "    #for image_name in image_list:\n",
    "    for image_name in image_files:\n",
    "        img = cv2.imread(os.path.join(IMAGE_FOLDER,image_name))\n",
    "        saliency_map_dict[image_name] = {}\n",
    "\n",
    "        # image_num = image_name[0:4]\n",
    "        # initialize\n",
    "        # imgsize = img.shape\n",
    "        # img_width = imgsize[1]\n",
    "        # img_height = imgsize[0]\n",
    "        # sm = pySaliencyMap.pySaliencyMap(img_width, img_height)\n",
    "        # computation of saliency maps\n",
    "        saliency_map = ITTISaliencyLib.get_spatial_saliency_map(img)\n",
    "        saliency_map_intensity = ITTISaliencyLib.get_spatial_saliency_map_I(img)\n",
    "        saliency_map_color = ITTISaliencyLib.get_spatial_saliency_map_C(img)\n",
    "        saliency_map_orientation = ITTISaliencyLib.get_spatial_saliency_map_O(img)\n",
    "\n",
    "        list_of_maps = [\n",
    "            (\"saliency_map\", saliency_map),\n",
    "            (\"saliency_map_intensity\", saliency_map_intensity),\n",
    "            (\"saliency_map_color\", saliency_map_color),\n",
    "            (\"saliency_map_orientation\", saliency_map_orientation),\n",
    "        ]\n",
    "\n",
    "        saliency_map_dict[image_name].update(list_of_maps)\n",
    "\n",
    "    return saliency_map_dict\n",
    "\n",
    "SALIENCY_MAP_DICT = make_saliency_map_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c30fa706-aa82-4869-bc9e-160f289fa339",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_semantic_saliency_fix(subject_num, group_input):\n",
    "    # Add necessary path\n",
    "    #fix_data_path = (\n",
    "    #    #\"/Users/patrikpolgari/Documents/Postdoc_Marburg/student_supervision/Teaching/B-NP2_Übung/Code/Task_C/data/derivatives/\"\n",
    "    #    RESULTS_FOLDER_FIX\n",
    "    #    + subject_num\n",
    "    #    + \"_fix_data.csv\"\n",
    "    #)\n",
    "    if group_input == \"A\":\n",
    "        fix_data_path = (\n",
    "        RESULTS_FOLDER_FIX \n",
    "            +\"group_A/\"\n",
    "            + subject_num\n",
    "            + \"_fix_data.csv\"\n",
    "    )\n",
    "    if group_input== \"B\":\n",
    "        fix_data_path = (\n",
    "        RESULTS_FOLDER_FIX \n",
    "            +\"group_B/\"\n",
    "            + subject_num\n",
    "            + \"_fix_data.csv\"\n",
    "    )\n",
    "\n",
    "    OSIE_data_path = OSIE_FILE#\"/Users/patrikpolgari/Documents/Postdoc_Marburg/student_supervision/Teaching/B-NP2_Übung/Code/Task_C/OSIE_40/attrs_OSIE_40.mat\"\n",
    "\n",
    "    #saliency_map_path = SALIENCY_MAP_DICT#\"/Users/patrikpolgari/Documents/Postdoc_Marburg/student_supervision/Teaching/B-NP2_Übung/Code/Task_C/OSIE_40/saliency_map_dict.npy\"\n",
    "\n",
    "    # Load the MATLAB .mat file containing the data\n",
    "    fix_data = pd.read_csv(fix_data_path, sep=\",\")\n",
    "    # mat_data = loadmat(OSIE_data_path)\n",
    "    mat_dict = loadmat(OSIE_data_path, simplify_cells=True)\n",
    "    attrs_OSIE_40 = mat_dict[\"attrs_OSIE_40\"]\n",
    "    #saliency_map_dict = np.load(\n",
    "    #    saliency_map_path,\n",
    "    #    allow_pickle=\"TRUE\",\n",
    "    #).item()\n",
    "    saliency_map_dict = SALIENCY_MAP_DICT\n",
    "    \n",
    "\n",
    "    # print(saliency_map_dict)\n",
    "    # df = pd.read_table(\"/Users/patrikpolgari/Documents/Postdoc_Marburg/student_supervision/Teaching/B-NP2_Übung/Code/Task_C/pilot_res/pp_scan_2024_03_21_16_08.asc\")\n",
    "    # ascii_grid = np.loadtxt(\"/Users/patrikpolgari/Documents/Postdoc_Marburg/student_supervision/Teaching/B-NP2_Übung/Code/Task_C/pilot_res/pp_scan_2024_03_21_16_08.asc\", skiprows=100)\n",
    "\n",
    "    # Constants and parameters\n",
    "    dims = [0, 1, 5, 7, 10]\n",
    "    # Header = [\"Text\", \"Face\", \"Taste\", \"Motion\", \"Touched\"]\n",
    "\n",
    "    im_width, im_height = 800, 600\n",
    "\n",
    "    im_width_real, im_height_real = 1146, 876  # real size\n",
    "    exp_scale_width = im_width_real / im_width  # 1.4325\n",
    "    exp_scale_height = im_height_real / im_height  # 1.46\n",
    "    screen_width, screen_height = 1920, 1080\n",
    "    # screen_x_cm, screen_y_cm, vp_dist_cm = 51.5, 29.0, 60\n",
    "    # sample_freq = 1000\n",
    "    # XPIX2DEG = np.arctan((screen_x_cm / screen_width) / vp_dist_cm) * (180 / np.pi)\n",
    "    # YPIX2DEG = np.arctan((screen_y_cm / screen_height) / vp_dist_cm) * (180 / np.pi)\n",
    "    # folder_name = f\"e1v{subject_num}b1\"\n",
    "    fixations = {}\n",
    "\n",
    "    # im_num = '1001.jpg'\n",
    "    # block_num = 1\n",
    "\n",
    "    BLOCK_NUMS = 40\n",
    "    for block_num in range(0, BLOCK_NUMS):\n",
    "        # block_num = 1\n",
    "        image_name = attrs_OSIE_40[block_num][\"img\"]\n",
    "        # objs = attrs_OSIE_40[block_num][\"objs\"]\n",
    "\n",
    "        fix_data_block = fix_data[fix_data[\"img\"] == image_name]\n",
    "        Cfix = len(fix_data_block)  # fixation count\n",
    "        Dfix = list(fix_data_block[\"dur\"])  # fixation durations\n",
    "        Xfix = list(\n",
    "            ((fix_data_block[\"axp\"] - (screen_width / 2)) / exp_scale_width) + (im_width / 2)\n",
    "        )\n",
    "        # Xfix = Xfix - (screen_width/2) #/ exp_scale_width)\n",
    "        # Xfix = Xfix + (im_width/2)\n",
    "        # Xfix_norm = Xfix/exp_scale_width # X coord\n",
    "        Yfix = list(\n",
    "            ((fix_data_block[\"ayp\"] - (screen_height / 2)) / exp_scale_height) + (im_height / 2)\n",
    "        )  # / exp_scale_height)  # Y coord\n",
    "        Order = list(fix_data_block[\"fix_order\"])\n",
    "        # for im_num in pd.unique(fix_data['img']):\n",
    "\n",
    "        # Analyze semantic content\n",
    "        sem = np.zeros((Cfix, len(dims)))\n",
    "\n",
    "        for d in range(len(dims)):\n",
    "            map_sum = np.zeros((im_height, im_width))\n",
    "            # map_ori = []\n",
    "            for obj in range(len(attrs_OSIE_40[block_num][\"objs\"])):\n",
    "                # obj = 0\n",
    "                if attrs_OSIE_40[block_num][\"objs\"][obj][\"features\"][dims[d]] > 0:\n",
    "                    # print(attrs_OSIE_40[block_num-1]['objs'][obj]['features'][dims[d]])\n",
    "                    map_ori = attrs_OSIE_40[block_num][\"objs\"][obj][\"map\"]\n",
    "                    # print(map_ori)\n",
    "                    map_sum += map_ori\n",
    "                    # print(map_sum)\n",
    "            for f in range(Cfix):\n",
    "                sem[f, d] = 0\n",
    "                if 0 < round(Xfix[f]) <= im_width and 0 < round(Yfix[f]) <= im_height:\n",
    "                    sem[f, d] = map_sum[int(round(Yfix[f])), int(round(Xfix[f]))] * Dfix[f]\n",
    "\n",
    "        sem_df = pd.DataFrame(sem)\n",
    "\n",
    "        # Analyze saliency content\n",
    "        sal = np.zeros((Cfix, len(saliency_map_dict[image_name])))\n",
    "        list_of_sal_maps = list(saliency_map_dict[image_name].keys())\n",
    "\n",
    "        for i, sal_map in enumerate(list_of_sal_maps):\n",
    "            # print(i, sal_map)\n",
    "            sal_map_ori = saliency_map_dict[image_name][sal_map]\n",
    "\n",
    "            for f in range(Cfix):\n",
    "                sal[f, i] = 0\n",
    "                if 0 < round(Xfix[f]) <= im_width and 0 < round(Yfix[f]) <= im_height:\n",
    "                    # sal[f, i] = sal_map_ori[int(round(Yfix[f])), int(round(Xfix[f]))] * Dfix[f]\n",
    "                    sal[f, i] = (\n",
    "                        np.sum(sal_map_ori < sal_map_ori[int(round(Yfix[f])), int(round(Xfix[f]))])\n",
    "                        / sal_map_ori.size\n",
    "                    )  # * Dfix[f]\n",
    "\n",
    "        sal_df = pd.DataFrame(sal, columns=list_of_sal_maps)\n",
    "\n",
    "        fixations[image_name] = {\n",
    "            \"X\": Xfix,\n",
    "            \"Y\": Yfix,\n",
    "            # \"Onset\": Tfix,\n",
    "            \"Duration\": Dfix,\n",
    "            # \"Amplitude\": Asac,\n",
    "            \"Sem\": sem_df,\n",
    "            \"Sal\": sal_df,\n",
    "            \"Order\": Order,\n",
    "        }\n",
    "\n",
    "    return fixations\n",
    "\n",
    "\n",
    "\n",
    "def extract_fix_split_half(group_input):\n",
    "    # fixations_odd = {}\n",
    "    # fixations_even = {}\n",
    "    if group_input=='A':\n",
    "        SUBJECT_LIST = [sub[:2] for sub in os.listdir(RESULTS_FOLDER_FIX+\"group_A/\")if sub != '.DS_Store']\n",
    "        \n",
    "    else:\n",
    "        SUBJECT_LIST = [sub[:2] for sub in os.listdir(RESULTS_FOLDER_FIX+\"group_B/\")if sub != '.DS_Store']\n",
    "    \n",
    "    fix_dataframe_group = pd.DataFrame()\n",
    "    #subject_list = [\"zt\", \"pp\", \"jp\"]\n",
    "    \n",
    "    #for subject_num in subject_list:\n",
    "    for subject_num in SUBJECT_LIST:\n",
    "        # subject_num='zt'\n",
    "        # fixations = extract_semantic_fix(subject_num)\n",
    "        fixations = extract_semantic_saliency_fix(subject_num, group_input)\n",
    "        # Constants and parameters\n",
    "        dims = [0, 1, 5, 7, 10]\n",
    "        header_sem1 = [\"Text1\", \"Face1\", \"Taste1\", \"Motion1\", \"Touched1\"]\n",
    "        header_sal1 = [\"Saliency1\", \"Intensity1\", \"Color1\", \"Orientation1\"]\n",
    "        header_sem2 = [\"Text2\", \"Face2\", \"Taste2\", \"Motion2\", \"Touched2\"]\n",
    "        header_sal2 = [\"Saliency2\", \"Intensity2\", \"Color2\", \"Orientation2\"]\n",
    "        header_tot = header_sem1 + header_sem2 + header_sal1 + header_sal2\n",
    "\n",
    "        first_fix = []\n",
    "        all_fix_sem_1 = pd.DataFrame()\n",
    "        all_fix_sal_1 = pd.DataFrame()\n",
    "        all_fix_sem_2 = pd.DataFrame()\n",
    "        all_fix_sal_2 = pd.DataFrame()\n",
    "        # d_fix = []\n",
    "        all_dur_1 = []  # pd.DataFrame()\n",
    "        all_dur_2 = []\n",
    "        fixation_keys = list(fixations.keys())\n",
    "\n",
    "        for i, key in enumerate(fixation_keys):\n",
    "            # key = '1001.jpg'\n",
    "            if fixations[key] is not None:\n",
    "                # sem\n",
    "                df_sem = pd.DataFrame(fixations[key][\"Sem\"])\n",
    "                df_sal = fixations[key][\"Sal\"]\n",
    "                df_dur = fixations[key][\"Duration\"]\n",
    "\n",
    "                if i % 2 == 0:\n",
    "                    all_fix_sem_2 = pd.concat([all_fix_sem_2, df_sem])\n",
    "                    all_dur_2 += df_dur\n",
    "                    all_fix_sal_2 = pd.concat([all_fix_sal_2, df_sal])\n",
    "\n",
    "                else:\n",
    "                    all_fix_sem_1 = pd.concat([all_fix_sem_1, df_sem])\n",
    "                    all_dur_1 += df_dur\n",
    "                    all_fix_sal_1 = pd.concat([all_fix_sal_1, df_sal])\n",
    "\n",
    "        # first_prop = np.sum(np.sign(first_fix), axis=0) / len(first_fix)\n",
    "        # all_prop = np.sum(all_fix, axis=0) / np.sum(d_fix)\n",
    "        all_prop_sem_1 = all_fix_sem_1.sum() / sum(all_dur_1)\n",
    "        all_prop_sem_2 = all_fix_sem_2.sum() / sum(all_dur_2)\n",
    "        all_prop_sal_1 = all_fix_sal_1.mean()\n",
    "        all_prop_sal_2 = all_fix_sal_2.mean()\n",
    "\n",
    "        # sem_fix_data = pd.DataFrame((np.array(header), first_prop, np.array(all_prop))).T.rename(columns={0:'header', 1:'first_prop', 2:'all_prop'})\n",
    "        fix_data_ind = {\n",
    "            # \"header\": header_sem1 + header_sem2 + header_sal1 + header_sal2,\n",
    "            # \"first_prop\": list(first_prop),\n",
    "            \"all_prop\": list(all_prop_sem_1)\n",
    "            + list(all_prop_sem_2)\n",
    "            + list(all_prop_sal_1)\n",
    "            + list(all_prop_sal_2),\n",
    "        }\n",
    "        fix_dataframe_ind = pd.DataFrame.from_dict(\n",
    "            fix_data_ind, orient=\"index\", columns=header_tot\n",
    "        )  # , columns=[\"header\", \"all_prop\"]).T\n",
    "        fix_dataframe_ind[\"sub_num\"] = subject_num\n",
    "        # fix_dataframe_ind(columns=\"header\")\n",
    "        fix_dataframe_group = pd.concat([fix_dataframe_group, fix_dataframe_ind])\n",
    "        \n",
    "    return fix_dataframe_group\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40b71f-610f-47e5-81b3-0f332c84a74b",
   "metadata": {},
   "source": [
    "<font size=3>To give a name (e.g. 'df') to and visualize the dataframe use the **extract_fix_split_half()** function. \n",
    "Note that this function has a parameter, your group ('A' or 'B', as character):\n",
    "\n",
    "```python\n",
    "df = extract_fix_split_half('group')\n",
    "display(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1c3d3-0fa4-4963-ba3b-12010a02d742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccd7731b-b361-4c74-884b-4e61c4d92090",
   "metadata": {},
   "source": [
    "<font size=3>To easily access two columns and give them each a name:\n",
    "```python\n",
    "Feature1 = df['Feature1']\n",
    "Feature2 = df['Feature2']\n",
    "print(Feature1)\n",
    "print(Feature2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813ba19-0e24-41af-a718-9fdbfd156fd5",
   "metadata": {},
   "source": [
    "<font size=3> Try yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6e72f-2af9-47f5-a8dd-445819d1c8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c8d2e6d-3b82-4740-b7f9-2765c2e6e355",
   "metadata": {},
   "source": [
    "<font size=3>You can calculate and print the Pearson correlation coefficient and the corresponding p-value as follows:\n",
    "```python\n",
    "R_feat, p_feat = scipy.stats.pearsonr(Feature1, Feature2)\n",
    "print(R_feat, p_feat)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d726e-035a-4afb-a1e3-e4739f7e621a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57f4b4fe-ebb3-4507-a722-0a8e4537148a",
   "metadata": {},
   "source": [
    "You can use the cells like a calculator, doing simple operations like:\n",
    "- additions:\n",
    "```python\n",
    "sum = a + b\n",
    "```\n",
    "- substractions:\n",
    "```python\n",
    "difference = a - b\n",
    "```\n",
    "- multiplications:\n",
    "```python\n",
    "product = a * b\n",
    "```\n",
    "- divisions:\n",
    "```python\n",
    "quotient = a / b\n",
    "```\n",
    "- or a combination of any of these..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec22bd-ac76-46df-a16b-2f1016f1165d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
